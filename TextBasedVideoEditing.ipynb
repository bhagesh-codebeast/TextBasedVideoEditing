{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+EJ0QpsiptBii+tJ1BP/a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhagesh-codebeast/TextBasedVideoEditing/blob/main/TextBasedVideoEditing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp3pQawiYv3a"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "!pip install -q --progress-bar off torch transformers gradio_client==0.2.7 gradio==3.35.2 datasets librosa ffmpeg-python python-dotenv aiohttp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import base64\n",
        "import ffmpeg\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import datasets\n",
        "import gradio as gr\n",
        "from pathlib import Path\n",
        "from difflib import Differ\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "WhgFuW7cydL-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = \"facebook/wav2vec2-base-960h\"\n",
        "cuda = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "videos_out_path = Path(\"./videos_out\")\n",
        "videos_out_path.mkdir(parents=True, exist_ok=True)"
      ],
      "metadata": {
        "id": "xGG9oQShyf9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Run this block twice to load models\n",
        "speech_recognizer = pipeline(task=\"automatic-speech-recognition\", model=f'{MODEL}', tokenizer=f'{MODEL}', framework=\"pt\", device=device)"
      ],
      "metadata": {
        "id": "f4H2wT_y-7z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def speech_to_text(video_file_path):\n",
        "  \"\"\"\n",
        "  Takes a video path to convert to audio, transcribe audio channel to text and char timestamps\n",
        "  \"\"\"\n",
        "  if (video_file_path == None):\n",
        "    raise ValueError(\"Error no video input\")\n",
        "  video_path = Path(video_file_path)\n",
        "  # convert video to audio 16k using PIPE to audio_memory\n",
        "  try:\n",
        "    audio_memory, _ = ffmpeg.input(video_path).output('-', format=\"wav\", ac=1, ar='16k').overwrite_output().global_args('-loglevel', 'quiet').run(capture_stdout=True)\n",
        "  except Exception as e:\n",
        "    raise RuntimeError(\"Error converting video to audio\")\n",
        "  last_time = time.time()\n",
        "  try:\n",
        "    output = speech_recognizer(audio_memory, return_timestamps=\"char\", chunk_length_s=10, stride_length_s=(4, 2))\n",
        "    transcription = output[\"text\"].lower()\n",
        "    timestamps = [[chunk[\"text\"].lower(), chunk[\"timestamp\"][0].tolist(), chunk[\"timestamp\"][1].tolist()] for chunk in output['chunks']]\n",
        "    return (transcription, transcription, timestamps)\n",
        "  except Exception as e:\n",
        "    raise RuntimeError(\"Error Running inference with local model\", e)\n",
        "\n",
        "async def cut_timestamps_to_video(video_in, transcription, text_in, timestamps):\n",
        "  \"\"\"\n",
        "  Given original video input, text transcript + timestamps, and edit ext cuts video segments into a single video\n",
        "  \"\"\"\n",
        "  video_path = Path(video_in)\n",
        "  video_file_name = video_path.stem\n",
        "  if (video_in == None or text_in == None or transcription == None):\n",
        "    raise ValueError(\"Inputs undefined\")\n",
        "  d = Differ()\n",
        "  # compare original transcription with edit text\n",
        "  diff_chars = d.compare(transcription, text_in)\n",
        "  # remove all text aditions from diff\n",
        "  filtered = list(filter(lambda x: x[0] != '+', diff_chars))\n",
        "  # groupping character timestamps so there are less cuts\n",
        "  idx = 0\n",
        "  grouped = {}\n",
        "  for (a, b) in zip(filtered, timestamps):\n",
        "    if a[0] != '-':\n",
        "      if idx in grouped:\n",
        "        grouped[idx].append(b)\n",
        "      else:\n",
        "        grouped[idx] = []\n",
        "        grouped[idx].append(b)\n",
        "    else:\n",
        "      idx += 1\n",
        "  # after grouping, gets the lower and upter start and time for each group\n",
        "  timestamps_to_cut = [[v[0][1], v[-1][2]] for v in grouped.values()]\n",
        "  between_str = '+'.join(map(lambda t: f'between(t,{t[0]},{t[1]})', timestamps_to_cut))\n",
        "  if timestamps_to_cut:\n",
        "    video_file = ffmpeg.input(video_in)\n",
        "    video = video_file.video.filter(\"select\", f'({between_str})').filter(\"setpts\", \"N/FRAME_RATE/TB\")\n",
        "    audio = video_file.audio.filter(\"aselect\", f'({between_str})').filter(\"asetpts\", \"N/SR/TB\")\n",
        "    output_video = f'./videos_out/{video_file_name}.mp4'\n",
        "    ffmpeg.concat(video, audio, v=1, a=1).output(output_video).overwrite_output().global_args('-loglevel', 'quiet').run()\n",
        "  else:\n",
        "    output_video = video_in\n",
        "  tokens = [(token[2:], token[0] if token[0] != \" \" else None)for token in filtered]\n",
        "  return (tokens, output_video)\n",
        "\n",
        "# ---- Gradio Layout -----\n",
        "video_in = gr.Video(label=\"Video file\", elem_id=\"video-container\")\n",
        "text_in = gr.Textbox(label=\"Transcription\", lines=10, interactive=True)\n",
        "video_out = gr.Video(label=\"Video Out\")\n",
        "diff_out = gr.HighlightedText(label=\"Cut Diffs\", combine_adjacent=True)\n",
        "\n",
        "css = \"\"\"\n",
        "#edit_btn, #reset_btn { align-self:stretch; }\n",
        "#\\\\31 3 { max-width: 540px; }\n",
        ".output-markdown {max-width: 65ch !important;}\n",
        "#video-container{ align-self:stretch; }\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "  transcription_var = gr.State()\n",
        "  timestamps_var = gr.State()\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      gr.Markdown(\"\"\"\n",
        "      # Edit Video By Editing Text\n",
        "      This project is a simple video editor where the edits are made by editing the audio transcription.\n",
        "      ## Upload Video\n",
        "      \"\"\")\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      video_in.render()\n",
        "      transcribe_btn = gr.Button(\"Transcribe\", elem_id=\"transcribe_btn\")\n",
        "      transcribe_btn.click(speech_to_text, [video_in], [text_in, transcription_var, timestamps_var])\n",
        "  with gr.Row():\n",
        "    with gr.Column():\n",
        "      gr.Markdown(\"\"\"## Edit Transcript\n",
        "      Edit the text below (only cuts, not additions).\"\"\")\n",
        "      text_in.render()\n",
        "      with gr.Row():\n",
        "        cut_btn = gr.Button(\"Edit Video\", elem_id=\"edit_btn\")\n",
        "        # send audio path and hidden variables\n",
        "        cut_btn.click(cut_timestamps_to_video, [video_in, transcription_var, text_in, timestamps_var], [diff_out, video_out])\n",
        "        reset_transcription = gr.Button(\"Reset Trascription\", elem_id=\"reset_btn\")\n",
        "        reset_transcription.click(lambda x: x, transcription_var, text_in)\n",
        "    with gr.Column():\n",
        "      gr.Markdown(\"\"\"## Output Video\"\"\")\n",
        "      video_out.render()\n",
        "      diff_out.render()\n",
        "demo.queue()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "4xfuEj4rZr-g",
        "outputId": "22a56298-4e48-424c-8666-97b4e7afab7c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://08d620828ee8c83301.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://08d620828ee8c83301.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://08d620828ee8c83301.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OJ3w0OAvznvM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}